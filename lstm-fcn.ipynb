{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom tqdm.auto import tqdm\n\nimport torch\nimport torch.autograd as autograd\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\n\nimport scipy.io as sio\nimport time\nimport os\n\nimport matplotlib.pyplot as plt\nimport seaborn as sn","metadata":{"id":"NKPoS5uz8Huo","execution":{"iopub.status.busy":"2023-09-21T10:31:37.323344Z","iopub.execute_input":"2023-09-21T10:31:37.323765Z","iopub.status.idle":"2023-09-21T10:31:43.361641Z","shell.execute_reply.started":"2023-09-21T10:31:37.323736Z","shell.execute_reply":"2023-09-21T10:31:43.360573Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(torch.__version__)\nprint(torch.cuda.is_available())\nprint(torch.cuda.get_device_name(0))\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(device)","metadata":{"id":"Rj5CPPF8A8TU","outputId":"ee345a7d-779c-4b0b-b060-18660a03ec11","execution":{"iopub.status.busy":"2023-09-21T10:31:43.366943Z","iopub.execute_input":"2023-09-21T10:31:43.369443Z","iopub.status.idle":"2023-09-21T10:31:43.424978Z","shell.execute_reply.started":"2023-09-21T10:31:43.369407Z","shell.execute_reply":"2023-09-21T10:31:43.423964Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"learning_rate = 1e-4\nnum_epoch = 30\nbatch_size = 128\ngan = False\nsampler = False","metadata":{"id":"OP-bzylgD8aU","execution":{"iopub.status.busy":"2023-09-21T10:31:43.429310Z","iopub.execute_input":"2023-09-21T10:31:43.431747Z","iopub.status.idle":"2023-09-21T10:31:43.440595Z","shell.execute_reply.started":"2023-09-21T10:31:43.431687Z","shell.execute_reply":"2023-09-21T10:31:43.439527Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataset","metadata":{"id":"GNlfPeRTHfpI"}},{"cell_type":"code","source":"class GPVSDataset(Dataset):\n    def __init__(self, df, feature_columns):\n        super().__init__()\n        self.df = df\n        self.sequences = self.df[feature_columns]\n        self.labels = self.df['Fault_type']\n\n    def __len__(self):\n        return len(self.sequences)\n\n    def __getitem__(self, idx):\n        sequence = np.array([np.array(item) for item in self.sequences.iloc[idx, :]]).T\n        label = self.labels[idx]\n        return dict(\n            sequence = torch.Tensor(sequence),\n            label = torch.tensor(label).long()\n        )","metadata":{"id":"8hftRKTb8Qu4","execution":{"iopub.status.busy":"2023-09-21T10:31:43.447349Z","iopub.execute_input":"2023-09-21T10:31:43.448785Z","iopub.status.idle":"2023-09-21T10:31:43.460909Z","shell.execute_reply.started":"2023-09-21T10:31:43.448751Z","shell.execute_reply":"2023-09-21T10:31:43.459638Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset_folder = '/kaggle/input/gpvs-gan-dataset/gpvs_sl200_s15'\nfilenames = os.listdir(dataset_folder)\n\nif gan:\n    train_df = pd.read_pickle(os.path.join(dataset_folder,[filename for filename in filenames if 'GAN' in filename][0]))\nelse:\n    train_df = pd.read_pickle(os.path.join(dataset_folder,[filename for filename in filenames if 'TRAIN' in filename][0]))\n\nval_df = pd.read_pickle(os.path.join(dataset_folder,[filename for filename in filenames if 'VALI' in filename][0]))\ntest_df = pd.read_pickle(os.path.join(dataset_folder,[filename for filename in filenames if 'TEST' in filename][0]))\n\nle = LabelEncoder()\nle.fit(train_df['Fault_type'])\n\ntrain_df['Fault_type'] = le.transform(train_df['Fault_type'])\nval_df['Fault_type'] = le.transform(val_df['Fault_type'])\ntest_df['Fault_type'] = le.transform(test_df['Fault_type'])","metadata":{"id":"oKOBk4nEITom","execution":{"iopub.status.busy":"2023-09-21T10:31:43.465844Z","iopub.execute_input":"2023-09-21T10:31:43.468220Z","iopub.status.idle":"2023-09-21T10:32:21.844501Z","shell.execute_reply.started":"2023-09-21T10:31:43.468184Z","shell.execute_reply":"2023-09-21T10:32:21.843488Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f'Train set lenght: {len(train_df)}')\nprint(f'Validation set lenght: {len(val_df)}')\nprint(f'Test set lenght: {len(test_df)}')","metadata":{"id":"DjdSeDgqKt3W","execution":{"iopub.status.busy":"2023-09-21T10:32:21.846108Z","iopub.execute_input":"2023-09-21T10:32:21.846448Z","iopub.status.idle":"2023-09-21T10:32:21.854278Z","shell.execute_reply.started":"2023-09-21T10:32:21.846417Z","shell.execute_reply":"2023-09-21T10:32:21.853361Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"le.classes_","metadata":{"execution":{"iopub.status.busy":"2023-09-21T10:32:21.855768Z","iopub.execute_input":"2023-09-21T10:32:21.856614Z","iopub.status.idle":"2023-09-21T10:32:21.865023Z","shell.execute_reply.started":"2023-09-21T10:32:21.856578Z","shell.execute_reply":"2023-09-21T10:32:21.863936Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# DataLoader","metadata":{}},{"cell_type":"code","source":"def getSampler(df, label_column):\n    class_occ = pd.DataFrame()\n    class_occ['Occ'] = df[label_column].value_counts().sort_index()\n    class_occ['weight'] = class_occ['Occ'].apply(lambda x: 1./x)\n    #print(class_occ)\n    \n    weights_dict = class_occ['weight'].to_dict()\n    df['weight'] = df[label_column].apply(lambda x: weights_dict[x])\n    #print(df)\n    \n    weights = torch.DoubleTensor(df.weight.values)\n    #print('weights: ', weights)\n\n    sampler = torch.utils.data.sampler.WeightedRandomSampler(weights, len(weights), replacement=True)\n    return sampler","metadata":{"execution":{"iopub.status.busy":"2023-09-21T10:32:21.866607Z","iopub.execute_input":"2023-09-21T10:32:21.867074Z","iopub.status.idle":"2023-09-21T10:32:21.877327Z","shell.execute_reply.started":"2023-09-21T10:32:21.867041Z","shell.execute_reply":"2023-09-21T10:32:21.876348Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature_columns = ['Ipv', 'Vpv', 'Vdc', 'ia', 'ib', 'ic', 'va', 'vb', 'vc', 'Iabc', 'If', 'Vabc', 'Vf']\n\ntrain_data = GPVSDataset(train_df, feature_columns)\nif sampler and not gan:\n    train_sampler = getSampler(train_df, 'Fault_type')\n    train_dataloader = DataLoader(train_data, batch_size=batch_size, sampler=train_sampler, shuffle=False)\nelse:\n    train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n\nval_data = GPVSDataset(val_df, feature_columns)\nval_dataloader = DataLoader(val_data, batch_size=batch_size)\n\ndataloaders = {\n   'train': train_dataloader,\n    'validation': val_dataloader\n}\n\ndataset_sizes = {x: len(dataloaders[x]) for x in ['train', 'validation']}","metadata":{"id":"bSByfhsSNj7P","execution":{"iopub.status.busy":"2023-09-21T10:32:21.879039Z","iopub.execute_input":"2023-09-21T10:32:21.879416Z","iopub.status.idle":"2023-09-21T10:32:21.909127Z","shell.execute_reply.started":"2023-09-21T10:32:21.879384Z","shell.execute_reply":"2023-09-21T10:32:21.908271Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def countLabels(dataloader, class_names):\n    labels_count = {x: 0 for x in class_names}\n\n    for data in dataloader:\n        labels = data['label']\n        for idx in range(len(class_names)):\n            labels_count[class_names[idx]] += torch.sum(labels == idx).item()\n        \n    return labels_count","metadata":{"execution":{"iopub.status.busy":"2023-09-21T10:32:21.912958Z","iopub.execute_input":"2023-09-21T10:32:21.913218Z","iopub.status.idle":"2023-09-21T10:32:21.919020Z","shell.execute_reply.started":"2023-09-21T10:32:21.913194Z","shell.execute_reply":"2023-09-21T10:32:21.917929Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#labels_count = countLabels(train_dataloader, le.classes_)\n#labels_count","metadata":{"execution":{"iopub.status.busy":"2023-09-21T10:32:21.920985Z","iopub.execute_input":"2023-09-21T10:32:21.921369Z","iopub.status.idle":"2023-09-21T10:32:21.928027Z","shell.execute_reply.started":"2023-09-21T10:32:21.921281Z","shell.execute_reply":"2023-09-21T10:32:21.926894Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{"id":"gQbd6zPHHbo-"}},{"cell_type":"code","source":"class LSTM_FCN(nn.Module):\n    def __init__(self, n_features, n_classes, n_hidden=256, n_layers=3):\n        super().__init__()\n\n        self.lstm = nn.LSTM(\n            input_size = n_features,\n            hidden_size = n_hidden,\n            batch_first = True,\n            num_layers = n_layers, # Stack LSTMs\n            dropout = 0.2  # This model works on a lot of regularisation\n        )\n\n        self.classifier = nn.Linear(n_hidden, n_classes)\n\n    def forward(self, x):\n        self.lstm.flatten_parameters()  # For distrubuted training\n\n        _, (hidden, _) = self.lstm(x)\n        # We want the output from the last layer to go into the final\n        # regressor linear layer\n        out = hidden[-1]\n\n        return self.classifier(out)","metadata":{"id":"GraYvu4H95WE","execution":{"iopub.status.busy":"2023-09-21T10:32:21.929580Z","iopub.execute_input":"2023-09-21T10:32:21.929930Z","iopub.status.idle":"2023-09-21T10:32:21.938778Z","shell.execute_reply.started":"2023-09-21T10:32:21.929899Z","shell.execute_reply":"2023-09-21T10:32:21.937190Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = LSTM_FCN(n_features=len(feature_columns), n_classes=len(le.classes_)).to(device)\n\ncriterion = torch.nn.CrossEntropyLoss()\noptimizer_ft = optim.Adam(model.parameters(), lr=learning_rate)\nscheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer_ft, patience = 3, verbose = True)","metadata":{"id":"g8KatzE7G522","execution":{"iopub.status.busy":"2023-09-21T10:32:21.940384Z","iopub.execute_input":"2023-09-21T10:32:21.941488Z","iopub.status.idle":"2023-09-21T10:32:27.165797Z","shell.execute_reply.started":"2023-09-21T10:32:21.941455Z","shell.execute_reply":"2023-09-21T10:32:27.164769Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#find best model on validation test\ndef train_loop_validation(dataloaders, startEpoch, numEpochs, model_conv, criterionCNN, optimizer_conv, scheduler,\n                          best_acc, best_loss, best_epoca, outputPath):\n  \n  for epochs in range(startEpoch, numEpochs + 1):\n    since = time.time()\n    \n    modelLoss_train = 0.0\n    modelAcc_train = 0.0\n\n    model_conv.train() \n\n    totalSize = 0\n    \n    #for each batch\n    for data in dataloaders['train']:\n        inputs = data['sequence'].type(torch.FloatTensor).to(device)\n        labels = data['label'].to(device)\n      \n        optimizer_conv.zero_grad()\n        model_conv.zero_grad()\n      \n        y = model_conv(inputs)\n        outp, preds = torch.max(y, 1)   \n        \n        lossCNN = criterionCNN(y, labels) #media per batch \n\n        modelLoss_train += lossCNN.item() * inputs.size(0)\n        totalSize += inputs.size(0)\n        modelAcc_train += torch.sum(preds == labels.data).item()\n\n        lossCNN.backward()  # pred = f(x)   -> loss = L(f(x), l_true)\n    \n        optimizer_conv.step()\n    \n    \n    modelLoss_epoch_train = modelLoss_train/totalSize\n    modelAcc_epoch_train  = modelAcc_train/totalSize\n    \n    #salvataggio dei pesi ad ogni iterazione -> nel caso si blocchi e vogliamo riprendere il train \n    torch.save(model_conv.state_dict(), outputPath + 'train_weights.pth')\n    \n    model_conv.eval()\n    totalSize_val = 0\n    modelLoss_val = 0.0\n    modelAcc_val = 0.0\n\n    for data in dataloaders['validation']:\n        inputs = data['sequence'].type(torch.FloatTensor).cuda()\n        labels = data['label'].cuda()\n      \n        y = model_conv(inputs)\n        outp, preds = torch.max(y, 1) \n        lossCNN = criterionCNN(y, labels)\n\n        modelLoss_val += lossCNN.item() * inputs.size(0)\n        totalSize_val += inputs.size(0)\n        modelAcc_val += torch.sum(preds == labels.data).item()\n    \n    modelLoss_epoch_val=modelLoss_val/totalSize_val\n    modelAcc_epoch_val = modelAcc_val/totalSize_val\n    time_elapsed = time.time()-since\n    \n    scheduler.step(modelLoss_epoch_val)\n\n    #print(time_elapsed)\n    print('[Epoch %d][TRAIN on %d [Loss: %.4f  ACC: %.4f]][VAL on %d [Loss: %.4f  ACC: %.4f]][TIME: %.0f m %.0f s]' \n          %(epochs, totalSize, modelLoss_epoch_train, modelAcc_epoch_train, totalSize_val, modelLoss_epoch_val, \n            modelAcc_epoch_val, time_elapsed // 60, time_elapsed % 60))\n    \n    #if epochs == 1 or modelLoss_epoch_val < best_loss:\n    if (modelAcc_epoch_val > best_acc) or (modelAcc_epoch_val == best_acc and modelLoss_epoch_val < best_loss) :\n        print('     .... Saving best weights ....')\n        best_acc = modelAcc_epoch_val\n        best_loss = modelLoss_epoch_val\n        best_epoca = epochs\n        #salvataggio dei migliori pesi sul validation\n        torch.save(model_conv.state_dict(), outputPath + 'best_model_weights.pth')\n      \n    \n    with open(outputPath + 'learningRate.txt', \"a\") as file_object:\n        file_object.write(str(optimizer_ft.param_groups[0]['lr']) +'\\n')\n    \n    with open(outputPath + 'lossTrain.txt', \"a\") as file_object:\n        file_object.write(str(modelLoss_epoch_train) +'\\n')\n      \n    with open(outputPath + 'AccTrain.txt', \"a\") as file_object:\n        file_object.write(str(modelAcc_epoch_train)+'\\n')\n      \n    with open(outputPath + 'lossVal.txt', \"a\") as file_object:\n        file_object.write(str(modelLoss_epoch_val)+'\\n')\n      \n    with open(outputPath + 'AccVal.txt', \"a\") as file_object:\n        file_object.write(str(modelAcc_epoch_val)+'\\n')\n      \n    sio.savemat(outputPath + 'check_point.mat', {'best_acc': best_acc, \n                                                 'best_loss': best_loss,\n                                                 'best_epoca': best_epoca,\n                                                 'last_epoch': epochs})","metadata":{"id":"-g_dA5jbHFJd","execution":{"iopub.status.busy":"2023-09-21T10:32:27.167167Z","iopub.execute_input":"2023-09-21T10:32:27.167511Z","iopub.status.idle":"2023-09-21T10:32:27.204046Z","shell.execute_reply.started":"2023-09-21T10:32:27.167475Z","shell.execute_reply":"2023-09-21T10:32:27.202855Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"startEpoch = 1\nbest_acc = 0\nbest_loss= 0\nbest_epoca = 0\nWeightPath = './'\ntrain_loop_validation(dataloaders, startEpoch, num_epoch, model, criterion, optimizer_ft, scheduler, best_acc, best_loss, best_epoca, WeightPath)","metadata":{"id":"iL5jfGlcHJnx","execution":{"iopub.status.busy":"2023-09-21T10:32:27.205779Z","iopub.execute_input":"2023-09-21T10:32:27.206127Z","iopub.status.idle":"2023-09-21T10:37:43.477851Z","shell.execute_reply.started":"2023-09-21T10:32:27.206094Z","shell.execute_reply":"2023-09-21T10:37:43.476778Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lossModel_Train = []\nlossModel_val = []\naccModel_Train = []\naccModel_val = []\nlrs = []\n\nWeightPath = './'\nfile = open(WeightPath + 'lossTrain.txt', 'r')\nTesto = file.readlines()\nfor element in Testo:\n    lossModel_Train.append(float(element))\n\nfile = open(WeightPath + 'lossVal.txt', 'r')\nTesto = file.readlines()\nfor element in Testo:\n    lossModel_val.append(float(element))\n\nplt.figure()\nplt.title(\"Model: Training Vs Validation Losses\")\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.plot(list(range(1,len(lossModel_Train)+1)), lossModel_Train, color='r', label=\"Training Loss\")\nplt.plot(list(range(1, len(lossModel_val)+1)), lossModel_val, color='g', label=\"Validation Loss\")\nplt.legend()\nplt.savefig(WeightPath + 'LossTrainVal.png')\n\nfile = open(WeightPath + 'AccTrain.txt', 'r')\nTesto = file.readlines()\nfor element in Testo:\n    accModel_Train.append(float(element))\n\nfile = open(WeightPath + 'AccVal.txt', 'r')\nTesto = file.readlines()\nfor element in Testo:\n    accModel_val.append(float(element))\n\nplt.figure()\nplt.title(\"Training Vs Validation Accuracies\")\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.plot(list(range(1, len(accModel_Train)+1)), accModel_Train, color='r', label=\"Training Accuracy\")\nplt.plot(list(range(1, len(accModel_val)+1)), accModel_val, color='g', label=\"Validation Accuracy\")\nplt.legend()\nplt.savefig(WeightPath + 'AccTrainVal.png')\n\nfile = open(WeightPath + 'learningRate.txt', 'r')\nTesto = file.readlines()\nfor element in Testo:\n    lrs.append(float(element))\n\nplt.figure()\nplt.title(\"Learning Rate\")\nplt.xlabel('Epoch')\nplt.ylabel('lr')\nplt.plot(list(range(1, len(lrs)+1)), lrs, color='b')\nplt.savefig(WeightPath + 'AccTrainVal.png')","metadata":{"id":"4wotfJm1HPFN","execution":{"iopub.status.busy":"2023-09-21T10:37:43.479260Z","iopub.execute_input":"2023-09-21T10:37:43.479692Z","iopub.status.idle":"2023-09-21T10:37:44.691611Z","shell.execute_reply.started":"2023-09-21T10:37:43.479657Z","shell.execute_reply":"2023-09-21T10:37:44.690649Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load best weights","metadata":{"id":"xDjeGnphPUcr"}},{"cell_type":"code","source":"weight_path = './best_model_weights.pth'\ncheckpoint = torch.load(weight_path)\nmodel.load_state_dict(checkpoint)","metadata":{"id":"jHpNQPSlHQL-","execution":{"iopub.status.busy":"2023-09-21T10:37:44.693142Z","iopub.execute_input":"2023-09-21T10:37:44.693497Z","iopub.status.idle":"2023-09-21T10:37:44.707559Z","shell.execute_reply.started":"2023-09-21T10:37:44.693463Z","shell.execute_reply":"2023-09-21T10:37:44.706674Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Test","metadata":{"id":"O6WNfBrtPSdP"}},{"cell_type":"code","source":"test_data = GPVSDataset(test_df.reset_index(drop=True), feature_columns)\ntest_dataloader = DataLoader(test_data, batch_size=1, shuffle=False)\n\ncorrect = 0\ntotal = 0\nmodel.eval()\nTest_results = pd.DataFrame()\n\n# since we're not training, we don't need to calculate the gradients for our outputs\nwith torch.no_grad():\n    for data in test_dataloader:\n        inputs, labels = data['sequence'], data['label']\n        inputs, labels = inputs.to(device), labels.to(device)\n\n        # calculate outputs by running images through the network\n        outputs = model(inputs)\n        # the class with the highest energy is what we choose as prediction\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        #print(predicted, labels, predicted == labels)\n        correct += (predicted == labels).sum().item()\n\n        Test_results = pd.concat([Test_results, pd.DataFrame({'label': [int(labels.item())], 'pred': [int(predicted.item())]})], ignore_index = True)\n\nprint(f'Accuracy of the network on the test sequences: {100 * correct // total} %')","metadata":{"id":"Tvqo_ljRHUMz","execution":{"iopub.status.busy":"2023-09-21T10:37:44.709018Z","iopub.execute_input":"2023-09-21T10:37:44.709367Z","iopub.status.idle":"2023-09-21T10:39:46.033442Z","shell.execute_reply.started":"2023-09-21T10:37:44.709334Z","shell.execute_reply":"2023-09-21T10:39:46.032407Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"c = confusion_matrix(Test_results['label'],Test_results['pred'])\ndf_cm = pd.DataFrame(c , index = le.classes_, columns = le.classes_)\nplt.figure(figsize = (20,20))\nplt.xlabel('Predicted Label')\nplt.ylabel('True Label')\nsn.heatmap(df_cm, annot=True)","metadata":{"id":"i0E6G-2FQW2h","execution":{"iopub.status.busy":"2023-09-21T10:39:46.034860Z","iopub.execute_input":"2023-09-21T10:39:46.035946Z","iopub.status.idle":"2023-09-21T10:39:47.212554Z","shell.execute_reply.started":"2023-09-21T10:39:46.035907Z","shell.execute_reply":"2023-09-21T10:39:47.211633Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(classification_report(Test_results['label'],Test_results['pred'], target_names = le.classes_))","metadata":{"id":"NmfX0G2WS2pp","execution":{"iopub.status.busy":"2023-09-21T10:39:47.213640Z","iopub.execute_input":"2023-09-21T10:39:47.214828Z","iopub.status.idle":"2023-09-21T10:39:47.255835Z","shell.execute_reply.started":"2023-09-21T10:39:47.214789Z","shell.execute_reply":"2023-09-21T10:39:47.254767Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}